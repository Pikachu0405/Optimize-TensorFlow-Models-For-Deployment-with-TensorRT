# Optimize-TensorFlow-Models-For-Deployment-with-TensorRT

1. Optimize Tensorflow models using TensorRT (TF-TRT)

2. Use TF-TRT to optimize several deep learning models at FP32, FP16, and INT8 precision 

3. Observe how tuning TF-TRT parameters affects performance and inference throughput


**Project Implementation and Takeaways --**

1. Setup your TensorFlow and TensorRT Runtime

2. Load the Data and Pre-trained InceptionV3 Model 

3. Create batched Input

4. Load the TensorFlow SavedModel

5. Get Baseline for Prediction Throughput and Accuracy

6. Convert a TensorFlow saved model into a TF-TRT Float32 Graph

7. Benchmark TF-TRT Float32

8. Convert to TF-TRT Float16 and Benchmark

9. Converting to TF-TRT INT8


**Project Award/Accomplishment Certificate:**[Optimize TensorFlow Models For Deployment with TensorRT.pdf](https://github.com/Pikachu0405/Optimize-TensorFlow-Models-For-Deployment-with-TensorRT/files/7660615/Optimize.TensorFlow.Models.For.Deployment.with.TensorRT.pdf)

![image](https://user-images.githubusercontent.com/93926742/144847359-bf3fbd97-0780-4c49-9fba-c3bbf28a4b51.png)
